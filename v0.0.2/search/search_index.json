{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Near Node Flash","text":"<p>Near Node Flash, also known as Rabbit, provides a disaggregated chassis-local storage solution which utilizes SR-IOV over a PCIe Gen 4.0 switching fabric to provide a set of compute blades with NVMe storage. It also provides a dedicated storage processor to offload tasks such as storage preparation and data movement from the compute nodes.</p> <p>Here you will find NNF User Guides, Examples, and Request For Comment (RFC) documents.</p>"},{"location":"guides/","title":"User Guides","text":""},{"location":"guides/#setup","title":"Setup","text":"<ul> <li>Initial Setup</li> <li>Compute Daemons</li> <li>Firmware Upgrade</li> <li>High Availability Cluster</li> <li>RBAC for Users</li> </ul>"},{"location":"guides/#provisioning","title":"Provisioning","text":"<ul> <li>Storage Profiles</li> <li>Data Movement Configuration</li> </ul>"},{"location":"guides/#nnf-user-containers","title":"NNF User Containers","text":"<ul> <li>User Containers</li> </ul>"},{"location":"guides/compute-daemons/readme/","title":"Compute Daemons","text":"<p>Rabbit software requires two daemons be installed and run on each compute node. Each daemon shares similar build, package, and installation processes described below.</p> <ul> <li>The Client Mount daemon, <code>clientmount</code>, provides the support for mounting Rabbit hosted file systems on compute nodes.</li> <li>The Data Movement daemon, <code>nnf-dm</code>, supports creating, monitoring, and managing data movement (copy-offload) operations</li> </ul>"},{"location":"guides/compute-daemons/readme/#building-from-source","title":"Building from source","text":"<p>Each daemon can be built in their respective repositories using the <code>build-daemon</code> make target. Go version &gt;= 1.19 must be installed to perform a local build.</p>"},{"location":"guides/compute-daemons/readme/#rpm-package","title":"RPM Package","text":"<p>Each daemon is packaged as part of the build process in GitHub. Source and Binary RPMs are available.</p>"},{"location":"guides/compute-daemons/readme/#installation","title":"Installation","text":"<p>For manual install, place the binary in the <code>/usr/bin/</code> directory.</p> <p>To install the application as a daemon service, run <code>/usr/bin/[BINARY-NAME] install</code></p>"},{"location":"guides/compute-daemons/readme/#authentication","title":"Authentication","text":"<p>NNF software defines a Kubernetes Service Account for granting communication privileges between the daemon and the kubeapi server. The token file and certificate file can be obtained by providing the necessary Service Account and Namespace to the below shell script.</p> Compute Daemon Service Account Namespace Client Mount dws-operator-controller-manager dws-operator-system Data Movement nnf-dm-controller-manager nnf-dm-system <pre><code>#!/bin/bash\nSERVICE_ACCOUNT=$1\nNAMESPACE=$2\nkubectl get secret ${SERVICE_ACCOUNT} -n ${NAMESPACE} -o json | jq -Mr '.data.token' | base64 --decode &gt; ./service.token\nkubectl get secret ${SERVICE_ACCOUNT} -n ${NAMESPACE} -o json | jq -Mr '.data[\"ca.crt\"]' | base64 --decode &gt; ./service.cert\n</code></pre> <p>The <code>service.token</code> and <code>service.cert</code> files must be copied to each compute node, typically in the <code>/etc/[BINARY-NAME]/</code> directory</p>"},{"location":"guides/compute-daemons/readme/#configuration","title":"Configuration","text":"<p>Installing the daemon will create a default configuration located at <code>/etc/systemd/system/[BINARY-NAME].service</code></p> <p>The command line arguments can be provided to the service definition or as an override file.</p> Argument Definition <code>--kubernetes-service-host=[ADDRESS]</code> The IP address or DNS entry of the kubeapi server <code>--kubernetes-service-port=[PORT]</code> The listening port of the kubeapi server <code>--service-token-file=[PATH]</code> Location of the service token file <code>--service-cert-file=[PATH]</code> Location of the service certificate file <code>--node-name=[COMPUTE-NODE-NAME]</code> Name of this compute node as described in the System Configuration. Defaults to the host name reported by the OS. <code>--nnf-node-name=[RABBIT-NODE-NAME]</code> <code>nnf-dm</code> daemon only. Name of the rabbit node connected to this compute node as described in the System Configuration. If not provided, the <code>--node-name</code> value is used to find the associated Rabbit node in the System Configuration. <code>--sys-config=[NAME]</code> <code>nnf-dm</code> daemon only. The System Configuration resource's name. Defaults to <code>default</code> <p>For example:</p> cat /etc/systemd/system/nnf-dm.service<pre><code>[Unit]\nDescription=Near-Node Flash (NNF) Data Movement Service\n\n[Service]\nPIDFile=/var/run/nnf-dm.pid\nExecStartPre=/bin/rm -f /var/run/nnf-dm.pid\nExecStart=/usr/bin/nnf-dm \\\n   --kubernetes-service-host=127.0.0.1 \\\n   --kubernetes-service-port=7777 \\\n   --service-token-file=/path/to/service.token \\\n   --service-cert-file=/path/to/service.cert \\\n   --node-name=this-compute-node \\\n   --nnf-node-name=my-rabbit-node\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"guides/compute-daemons/readme/#nnf-dm-specific-configuration","title":"nnf-dm Specific Configuration","text":"<p>nnf-dm has some additional configuration options that can be used to tweak the kubernetes client:</p> Argument Definition <code>--kubernetes-qps=[QPS]</code> The number of Queries Per Second (QPS) before client-side rate-limiting starts. Defaults to 50. <code>--kubernetes-burst=[QPS]</code> Once QPS is hit, allow this many concurrent calls. Defaults to 100."},{"location":"guides/compute-daemons/readme/#easy-deployment","title":"Easy Deployment","text":"<p>The nnf-deploy tool's <code>install</code> command can be used to run the daemons on a system's set of compute nodes. This option will compile the latest daemon binaries, retrieve the service token and certificates, and will copy and install the daemons on each of the compute nodes. Refer to the nnf-deploy repository and run <code>nnf-deploy install --help</code> for details.</p>"},{"location":"guides/data-movement/readme/","title":"Data Movement Configuration","text":"<p>Data Movement can be configured in multiple ways:</p> <ol> <li>Server side</li> <li>Per Copy Offload API Request arguments</li> </ol> <p>The first method is a \"global\" configuration - it affects all data movement operations. The second is done per the Copy Offload API, which allows for some configuration on a per-case basis, but is limited in scope. Both methods are meant to work in tandem.</p>"},{"location":"guides/data-movement/readme/#server-side-configmap","title":"Server Side ConfigMap","text":"<p>The server side configuration is done via the <code>nnf-dm-config</code> config map:</p> <pre><code>kubectl -n nnf-dm-system get configmap nnf-dm-config\n</code></pre> <p>The config map allows you to configure the following:</p> Setting Description slots The number of slots specified in the MPI hostfile. A value less than 1 disables the use of slots in the hostfile. maxSlots The number of max_slots specified in the MPI hostfile. A value less than 1 disables the use of max_slots in the hostfile. command The full command to execute data movement. More detail in the following section. progressIntervalSeconds interval to collect the progress data from the <code>dcp</code> command."},{"location":"guides/data-movement/readme/#command","title":"<code>command</code>","text":"<p>The full data movement <code>command</code> can be set here. By default, Data Movement uses <code>mpirun</code> to run <code>dcp</code> to perform the data movement. Changing the <code>command</code> is useful for tweaking <code>mpirun</code> or <code>dcp</code> options or to replace the command with something that can aid in debugging (e.g. <code>hostname</code>).</p> <p><code>mpirun</code> uses hostfiles to list the hosts to launch <code>dcp</code> on. This hostfile is created for each Data Movement operation, and it uses the config map to set the <code>slots</code> and <code>maxSlots</code> for each host (i.e. NNF node) in the hostfile. The number of <code>slots</code>/<code>maxSlots</code> is the same for every host in the hostfile.</p> <p>Additionally, Data Movement uses substitution to fill in dynamic information for each Data Movement operation. Each of these must be present in the command for Data Movement to work properly when using <code>mpirun</code> and <code>dcp</code>:</p> VAR Description <code>$HOSTFILE</code> hostfile that is created and used for mpirun. <code>$UID</code> User ID that is inherited from the Workflow. <code>$GID</code> Group ID that is inherited from the Workflow. <code>$SRC</code> source for the data movement. <code>$DEST</code> destination for the data movement. <p>By default, the command will look something like the following. Please see the config map itself for the most up to date default command:</p> <pre><code>mpirun --allow-run-as-root --hostfile $HOSTFILE dcp --progress 1 --uid $UID --gid $GID $SRC $DEST\n</code></pre>"},{"location":"guides/data-movement/readme/#profiles","title":"Profiles","text":"<p>Note: This feature is not fully implemented, but is present in the <code>nnf-dm-config</code> config map. Only the <code>default</code> profile is used, and it must be present in the configuration. Once the feature is implemented, a user will be able to select a profile using #DW directives and/or the Copy Offload API. Right now, users can add additional profiles into the config map, but the only way to use them would be to rename one of them to be <code>default</code>.</p> <p><code>slots</code>, <code>maxSlots</code>, and <code>command</code> can be stored in Data Movement profiles. These profiles are available for a quick way to switch between different settings for a particular workflow.</p> <p>Example profile:</p> <pre><code>profiles:\ndefault:\nslots: 8\nmaxSlots: 0\ncommand: mpirun --allow-run-as-root --hostfile $HOSTFILE dcp --progress 1 --uid $UID --gid $GID $SRC $DEST\n</code></pre>"},{"location":"guides/data-movement/readme/#copy-offload-api-daemon","title":"Copy Offload API Daemon","text":"<p>The <code>CreateRequest</code> API call that is used to create Data Movement with the Copy Offload API has some options to allow a user to specify some options for that particular Data Movement. These settings are on a per-request basis.</p> <p>See the CreateRequest API definition for what can be configured.</p>"},{"location":"guides/firmware-upgrade/readme/","title":"Firmware Upgrade Procedures","text":"<p>This guide presents the firmware upgrade procedures to upgrade firmware from the Rabbit using tools present in the operating system.</p>"},{"location":"guides/firmware-upgrade/readme/#pcie-switch-firmware-upgrade","title":"PCIe Switch Firmware Upgrade","text":"<p>In order to upgrade the firmware on the PCIe switch, the <code>switchtec</code> kernel driver and utility of the same name must be installed. Rabbit hardware consists of two PCIe switches, which can be managed by devices typically located at <code>/dev/switchtec0</code> and <code>/dev/switchtec1</code>.</p> <p>Danger</p> <p>Upgrading the switch firmware will cause the switch to reset. Prototype Rabbit units not supporting hotplug should undergo a power-cycle to ensure switch initialization following firmware uprade. Similarily, compute nodes not supporting hotplug may lose connectivity after firmware upgrade and should also be power-cycled.</p> <pre><code>IMAGE=$1 # Provide the path to the firmware image file\nSWITCHES=(\"/dev/switchtec0\" \"/dev/switchtec1\")\nfor SWITCH in \"${SWITCHES[@]}\"; do switchtec fw-update \"$SWITCH\" \"$IMAGE\" --yes; done\n</code></pre>"},{"location":"guides/firmware-upgrade/readme/#nvme-drive-firmware-upgrade","title":"NVMe Drive Firmware Upgrade","text":"<p>In order to upgrade the firmware on NVMe drives attached to Rabbit, the <code>switchtec</code> and <code>switchtec-nvme</code> executables must be installed. All firmware downloads to drives are sent to the physical function of the drive which is accessible only using the <code>switchtec-nvme</code> executable.</p>"},{"location":"guides/firmware-upgrade/readme/#batch-method","title":"Batch Method","text":""},{"location":"guides/firmware-upgrade/readme/#download-and-commit-new-firmware","title":"Download and Commit New Firmware","text":"<p>The nvme.sh helper script applies the same command to each physical device fabric ID in the system. It provides a convenient way to upgrade the firmware on all drives in the system. Please see fw-download and fw-commit for details about the individual commands.</p> <pre><code># Download firmware to all drives\n./nvme.sh cmd fw-download --fw=&lt;/path/to/nvme.fw&gt;\n\n# Commit the new firmware\n# action=3: The image is requested to be activated immediately\n./nvme.sh cmd fw-commit --action=3\n</code></pre>"},{"location":"guides/firmware-upgrade/readme/#rebind-the-pcie-connections","title":"Rebind the PCIe Connections","text":"<p>In order to use the drives at this point, they must be unbound and bound to the PCIe fabric to reset device connections. The bind.sh helper script performs these two actions. Its use is illustrated below.</p> <pre><code># Unbind all drives from the Rabbit to disconnect the PCIe connection to the drives\n./bind.sh unbind\n\n# Bind all drives to the Rabbit to reconnect the PCIe bus\n./bind.sh bind\n# At this point, your drives should be running the new firmware.\n# Verify the firmware...\n./nvme.sh cmd id-ctrl | grep -E \"^fr \"\n</code></pre>"},{"location":"guides/firmware-upgrade/readme/#individual-drive-method","title":"Individual Drive Method","text":""},{"location":"guides/firmware-upgrade/readme/#determine-physical-device-fabric-id","title":"Determine Physical Device Fabric ID","text":"<p>The first step is to determine a drive's unique Physical Device Fabric Identifier (PDFID). The following code fragment demonstrates one way to list the physcial device fabric ids of all the NVMe drives in the system.</p> <pre><code>#!/bin/bash\nSWITCHES=(\"/dev/switchtec0\" \"/dev/switchtec1\")\nfor SWITCH in \"${SWITCHES[@]}\";\ndo\nmapfile -t PDFIDS &lt; &lt;(sudo switchtec fabric gfms-dump \"${SWITCH}\" | grep \"Function 0 \" -A1 | grep PDFID | awk '{print $2}')\nfor INDEX in \"${!PDFIDS[@]}\";\ndo\necho \"${PDFIDS[$INDEX]}@$SWITCH\"\ndone\ndone\n</code></pre> <pre><code># Produces a list like this:\n0x1300@/dev/switchtec0\n0x1600@/dev/switchtec0\n0x1700@/dev/switchtec0\n0x1400@/dev/switchtec0\n0x1800@/dev/switchtec0\n0x1900@/dev/switchtec0\n0x1500@/dev/switchtec0\n0x1a00@/dev/switchtec0\n0x4100@/dev/switchtec1\n0x3c00@/dev/switchtec1\n0x4000@/dev/switchtec1\n0x3e00@/dev/switchtec1\n0x4200@/dev/switchtec1\n0x3b00@/dev/switchtec1\n0x3d00@/dev/switchtec1\n0x3f00@/dev/switchtec1\n</code></pre>"},{"location":"guides/firmware-upgrade/readme/#download-firmware","title":"Download Firmware","text":"<p>Using the physical device fabric identifier, the following commands update the firmware for specified drive.</p> <pre><code># Download firmware to the drive\nsudo switchtec-nvme fw-download &lt;PhysicalDeviceFabricID&gt; --fw=&lt;/path/to/nvme.fw&gt;\n\n# Activate the new firmware\n# action=3: The image is requested to be activated immediately without reset.\nsudo switchtec-nvme fw-commit --action=3\n</code></pre>"},{"location":"guides/firmware-upgrade/readme/#rebind-pcie-connection","title":"Rebind PCIe Connection","text":"<p>Once the firmware has been downloaded and committed, the PCIe connection from the Rabbit to the drive must be unbound and rebound. Please see bind.sh for details.</p>"},{"location":"guides/ha-cluster/notes/","title":"Notes","text":"<p>pcs stonith create stonith-rabbit-node-1 fence_nnf pcmk_host_list=rabbit-node-1 kubernetes-service-host=10.30.107.247 kubernetes-service-port=6443 service-token-file=/etc/nnf/service.token service-cert-file=/etc/nnf/service.cert nnf-node-name=rabbit-node-1 verbose=1</p> <p>pcs stonith create stonith-rabbit-compute-2 fence_redfish pcmk_host_list=\"rabbit-compute-2\" ip=10.30.105.237 port=80 systems-uri=/redfish/v1/Systems/1 username=root password=REDACTED ssl_insecure=true verbose=1</p> <p>pcs stonith create stonith-rabbit-compute-3 fence_redfish pcmk_host_list=\"rabbit-compute-3\" ip=10.30.105.253 port=80 systems-uri=/redfish/v1/Systems/1 username=root password=REDACTED ssl_insecure=true verbose=1</p>"},{"location":"guides/ha-cluster/readme/","title":"High Availability Cluster","text":"<p>NNF software supports provisioning of Red Hat GFS2 (Global File System 2) storage. Per RedHat:</p> <p>GFS2 allows multiple nodes to share storage at a block level as if the storage were connected locally to each cluster node. GFS2 cluster file system requires a cluster infrastructure.</p> <p>Therefore, in order to use GFS2, the NNF node and its associated compute nodes must form a high availability cluster.</p>"},{"location":"guides/ha-cluster/readme/#cluster-setup","title":"Cluster Setup","text":"<p>Red Hat provides instructions for creating a high availability cluster with Pacemaker, including instructions for installing cluster software and creating a high availability cluster. When following these instructions, each of the high availability clusters that are created should be named after the hostname of the NNF node. In the Red Hat examples the cluster name is <code>my_cluster</code>.</p>"},{"location":"guides/ha-cluster/readme/#fencing-agents","title":"Fencing Agents","text":"<p>Fencing is the process of restricting and releasing access to resources that a failed cluster node may have access to. Since a failed node may be unresponsive, an external device must exist that can restrict access to shared resources of that node, or to issue a hard reboot of the node. More information can be found form Red Hat: 1.2.1 Fencing.</p> <p>HPE hardware implements software known as the Hardware System Supervisor (HSS), which itself conforms to the SNIA Redfish/Swordfish standard. This provides the means to manage hardware outside the host OS.</p>"},{"location":"guides/ha-cluster/readme/#nnf-fencing","title":"NNF Fencing","text":""},{"location":"guides/ha-cluster/readme/#source","title":"Source","text":"<p>The NNF Fencing agent is available at https://github.com/NearNodeFlash/fence-agents under the <code>nnf</code> branch.</p> <pre><code>git clone https://github.com/NearNodeFlash/fence-agents --branch nnf\n</code></pre>"},{"location":"guides/ha-cluster/readme/#build","title":"Build","text":"<p>Refer to the <code>NNF.md file</code> at the root directory of the fence-agents repository. The fencing agents must be installed on every node in the cluster.</p>"},{"location":"guides/ha-cluster/readme/#setup","title":"Setup","text":"<p>Configure the NNF agent with the following parameters:</p> Argument Definition <code>kubernetes-service-host=[ADDRESS]</code> The IP address of the kubeapi server <code>kubernetes-service-port=[PORT]</code> The listening port of the kubeapi server <code>service-token-file=[PATH]</code> The location of the service token file. The file must be present on all nodes within the cluster <code>service-cert-file=[PATH]</code> The location of the service certificate file. The file must be present on all nodes within the cluster <code>nnf-node-name=[NNF-NODE-NAME]</code> Name of the NNF node as it is appears in the System Configuration <code>api-version=[VERSION]</code> The API Version of the NNF Node resource. Defaults to \"v1alpha1\" <p>The token and certificate can be found in the Kubernetes Secrets resource for the nnf-system/nnf-fencing-agent ServiceAccount. This provides RBAC rules to limit the fencing agent to only the Kubernetes resources it needs access to.</p> <p>For example, setting up the NNF fencing agent on <code>rabbit-node-1</code> with a kubernetes service API running at <code>192.168.0.1:6443</code> and the service token and certificate copied to <code>/etc/nnf/fence/</code>. This needs to be run on one node in the cluster.</p> <pre><code>pcs stonith create rabbit-node-1 fence_nnf pcmk_host_list=rabbit-node-1 kubernetes-service-host=192.168.0.1 kubernetes-service-port=6443 service-token-file=/etc/nnf/fence/service.token service-cert-file=/etc/nnf/fence/service.cert nnf-node-name=rabbit-node-1\n</code></pre>"},{"location":"guides/ha-cluster/readme/#recovery","title":"Recovery","text":"<p>Since the NNF node is connected to 16 compute blades, careful coordination around fencing of a NNF node is required to minimize the impact of the outage. When a Rabbit node is fenced, the corresponding DWS Storage resource (<code>storages.dws.cray.hpe.com</code>) status changes. The workload manager must observe this change and follow the procedure below to recover from the fencing status.</p> <ol> <li>Observed the <code>storage.Status</code> changed and that <code>storage.Status.RequiresReboot == True</code></li> <li>Set the <code>storage.Spec.State := Disabled</code></li> <li>Wait for a change to the Storage status <code>storage.Status.State == Disabled</code></li> <li>Reboot the NNF node</li> <li>Set the <code>storage.Spec.State := Enabled</code></li> <li>Wait for <code>storage.Status.State == Enabled</code></li> </ol>"},{"location":"guides/ha-cluster/readme/#compute-fencing","title":"Compute Fencing","text":"<p>The Redfish fencing agent from ClusterLabs should be used for Compute nodes in the cluster. It is also included at https://github.com/NearNodeFlash/fence-agents, and can be built at the same time as the NNF fencing agent. Configure the agent with the following parameters:</p> Argument Definition <code>ip=[ADDRESS]</code> The IP address or hostname of the HSS controller <code>port=80</code> The Port of the HSS controller. Must be <code>80</code> <code>systems-uri=/redfish/v1/Systems/1</code> The URI of the Systems object. Must be <code>/redfish/v1/Systems/1</code> <code>ssl-insecure=true</code> Instructs the use of an insecure SSL exchange. Must be <code>true</code> <code>username=[USER]</code> The user name for connecting to the HSS controller <code>password=[PASSWORD]</code> the password for connecting to the HSS controller <p>For example, setting up the Redfish fencing agent on <code>rabbit-compute-2</code> with the redfish service at <code>192.168.0.1</code>. This needs to be run on one node in the cluster.</p> <pre><code>pcs stonith create rabbit-compute-2 fence_redfish pcmk_host_list=rabbit-compute-2 ip=192.168.0.1 systems-uri=/redfish/v1/Systems/1 username=root password=password ssl_insecure=true\n</code></pre>"},{"location":"guides/ha-cluster/readme/#dummy-fencing","title":"Dummy Fencing","text":"<p>The dummy fencing agent from ClusterLabs can be used for nodes in the cluster for an early access development system.</p>"},{"location":"guides/ha-cluster/readme/#configuring-a-gfs2-file-system-in-a-cluster","title":"Configuring a GFS2 file system in a cluster","text":"<p>Follow steps 1-8 of the procedure from Red Hat: Configuring a GFS2 file system in a cluster.</p>"},{"location":"guides/initial-setup/readme/","title":"Initial Setup Instructions","text":"<p>Instructions for the initial setup of a Rabbit are included in this document.</p>"},{"location":"guides/initial-setup/readme/#lvm-configuration-on-rabbit","title":"LVM Configuration on Rabbit","text":"LVM Details <p>Running LVM commands (lvcreate/lvremove) on a Rabbit to create logical volumes is problematic if those commands run within a container. Rabbit Storage Orchestration   code contained in the <code>nnf-node-manager</code> Kubernetes pod executes LVM commands from within the container. The problem is that the LVM create/remove commands wait for a   UDEV confirmation cookie that is set when UDEV rules run within the host OS. These cookies are not synchronized with the containers where the LVM commands execute.</p> <p>3 options to solve this problem are:</p> <ol> <li>Disable UDEV sync at the host operating system level</li> <li>Disable UDEV sync using the <code>\u2013noudevsync</code> command option for each LVM command</li> <li>Clear the UDEV cookie using the <code>dmsetup udevcomplete_all</code> command after the lvcreate/lvremove command.</li> </ol> <p>Taking these in reverse order using option 3 above which allows UDEV settings within the host OS to remain unchanged from the default, one would need to start the   <code>dmsetup</code> command on a separate thread because the LVM create/remove command waits for the UDEV cookie. This opens too many error paths, so it was rejected.</p> <p>Option 2 allows UDEV settings within the host OS to remain unchanged from the default, but the use of UDEV within production Rabbit systems is viewed as unnecessary   because the host OS is PXE-booted onto the node vs loaded from an device that is discovered by UDEV.</p> <p>Option 1 above is what we chose to implement because it is the simplest. The following sections discuss this setting.</p> <p>In order for LVM commands to run within the container environment on a Rabbit, the following change is required to the <code>/etc/lvm/lvm.conf</code> file on Rabbit.</p> <pre><code>sed -i 's/udev_sync = 1/udev_sync = 0/g' /etc/lvm/lvm.conf\n</code></pre>"},{"location":"guides/initial-setup/readme/#zfs","title":"ZFS","text":"<p>ZFS kernel module must be enabled to run on boot. This can be done by creating a file, <code>zfs.conf</code>, containing the string \"zfs\" in your systems modules-load.d directory.</p> <pre><code>echo \"zfs\" &gt; /etc/modules-load.d/zfs.conf\n</code></pre>"},{"location":"guides/initial-setup/readme/#kubernetes-initial-setup","title":"Kubernetes Initial Setup","text":"<p>Installation of Kubernetes (k8s) nodes proceeds by installing k8s components onto the master node(s) of the cluster, then installing k8s components onto the worker nodes and joining those workers to the cluster. The k8s cluster setup for Rabbit requires 3 distinct k8s node types for operation:</p> <ul> <li>Master: 1 or more master nodes which serve as the Kubernetes API server and control access to the system. For HA, at least 3 nodes should be dedicated to this role.</li> <li>Worker: 1 or more worker nodes which run the system level controller manager (SLCM) and Data Workflow Services (DWS) pods. In production, at least 3 nodes should be dedicated to this role.</li> <li>Rabbit: 1 or more Rabbit nodes which run the node level controller manager (NLCM) code. The NLCM daemonset pods are exclusively scheduled on Rabbit nodes. All Rabbit nodes are joined to the cluster as k8s workers, and they are tainted to restrict the type of work that may be scheduled on them. The NLCM pod has a toleration that allows it to run on the tainted (i.e. Rabbit) nodes.</li> </ul>"},{"location":"guides/initial-setup/readme/#certificate-manager","title":"Certificate manager","text":"<p>Webhooks require the Jetstack <code>cert-manager</code>. Installation is shown below.</p> <pre><code>export certver=\"v1.7.0\"\n# Required for webhooks\nkubectl apply -f https://github.com/jetstack/cert-manager/releases/download/\"$certver\"/cert-manager.yaml\n</code></pre>"},{"location":"guides/initial-setup/readme/#kubernetes-node-labels","title":"Kubernetes Node Labels","text":"Node Type Node Label Generic Kubernetes Worker Node cray.nnf.manager=true Rabbit Node cray.nnf.node=true cray.nnf.x-name=$NODE"},{"location":"guides/initial-setup/readme/#kubernetes-node-taints","title":"Kubernetes Node Taints","text":"Node Type Node Label Rabbit Node cray.nnf.node=true:NoSchedule <p>See Taints and Tolerations. The NearNodeFlash/nnf-deploy/init.sh script provides examples of labeling and tainting k8s nodes for use with Rabbit.</p>"},{"location":"guides/initial-setup/readme/#rabbit-system-configuration","title":"Rabbit System Configuration","text":"<p>The SystemConfiguration Custom Resource Definition (CRD) is a DWS resource that describes the hardware layout of the whole system. It is expected that an administrator creates a single SystemConfiguration resource when the system is being set up. There is no need to update the SystemConfiguration resource unless hardware is added to or removed from the system.</p> System Configuration Details <p>Rabbit software looks for a SystemConfiguration named <code>default</code> in the <code>default</code> namespace. This resource contains a list of compute nodes and storage nodes, and it describes the mapping between them. There are two different consumers of the SystemConfiguration resource in the NNF software:</p> <p><code>NnfNodeReconciler</code> - The reconciler for the NnfNode resource running on the Rabbit nodes reads the SystemConfiguration resource. It uses the Storage to compute mapping information to fill in the HostName section of the NnfNode resource. This information is then used to populate the DWS Storage resource.</p> <p><code>NnfSystemConfigurationReconciler</code> - This reconciler runs in the <code>nnf-controller-manager</code>. It creates a Namespace for each compute node listed in the SystemConfiguration. These namespaces are used by the client mount code.</p> <p>Here is an example <code>SystemConfiguration</code>:</p> Spec Section Notes computeNodes List of names of compute nodes in the system storageNodes List of Rabbits and the compute nodes attached storageNodes[].type Must be \"Rabbit\" storageNodes[].computeAccess List of {slot, compute name} elements that indicate physical slot index that the named compute node is attached to <pre><code>apiVersion: dws.cray.hpe.com/v1alpha1\nkind: SystemConfiguration\nmetadata:\nname: default\nnamespace: default\nspec:\ncomputeNodes:\n- name: compute-01\n- name: compute-02\n- name: compute-03\n- name: compute-04\nstorageNodes:\n- computesAccess:\n- index: 0\nname: compute-01\n- index: 1\nname: compute-02\n- index: 6\nname: compute-03\nname: rabbit-name-01\ntype: Rabbit\n- computesAccess:\n- index: 4\nname: compute-04\nname: rabbit-name-02\ntype: Rabbit\n</code></pre>"},{"location":"guides/rbac-for-users/readme/","title":"RBAC: Role-Based Access Control","text":"<p>RBAC (Role Based Access Control) determines the operations a user or service can perform on a list of Kubernetes resources. RBAC affects everything that interacts with the kube-apiserver (both users and services internal or external to the cluster). More information about RBAC can be found in the Kubernetes documentation.</p>"},{"location":"guides/rbac-for-users/readme/#rbac-for-users","title":"RBAC for Users","text":"<p>This section shows how to create a kubeconfig file with RBAC set up to restrict access to view only for resources.</p>"},{"location":"guides/rbac-for-users/readme/#overview","title":"Overview","text":"<p>User access to a Kubernetes cluster is defined through a kubeconfig file. This file contains the address of the kube-apiserver as well as the key and certificate for the user. Typically this file is located in <code>~/.kube/config</code>. When a kubernetes cluster is created, a config file is generated for the admin that allows unrestricted access to all resources in the cluster. This is the equivalent of <code>root</code> on a Linux system.</p> <p>The goal of this document is to create a new kubeconfig file that allows view only access to Kubernetes resources. This kubeconfig file can be shared between the HPE employees to investigate issues on the system. This involves:</p> <ul> <li>Generating a new key/cert pair for an \"hpe\" user</li> <li>Creating a new kubeconfig file</li> <li>Adding RBAC rules for the \"hpe\" user to allow read access</li> </ul>"},{"location":"guides/rbac-for-users/readme/#generate-a-key-and-certificate","title":"Generate a Key and Certificate","text":"<p>The first step is to create a new key and certificate so that HPE employees can authenticate as the \"hpe\" user. This will likely be done on one of the master nodes. The <code>openssl</code> command needs access to the certificate authority file. This is typically located in <code>/etc/kubernetes/pki</code>.</p> <pre><code># make a temporary work space\nmkdir /tmp/rabbit\ncd /tmp/rabbit\n\n# Create this user\nexport USERNAME=hpe\n\n# generate a new key\nopenssl genrsa -out rabbit.key 2048\n# create a certificate signing request for this user\nopenssl req -new -key rabbit.key -out rabbit.csr -subj \"/CN=$USERNAME\"\n# generate a certificate using the certificate authority on the k8s cluster. This certificate lasts 500 days\nopenssl x509 -req -in rabbit.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out rabbit.crt -days 500\n</code></pre>"},{"location":"guides/rbac-for-users/readme/#create-a-kubeconfig","title":"Create a kubeconfig","text":"<p>After the keys have been generated, a new kubeconfig file can be created for this user. The admin kubeconfig <code>/etc/kubernetes/admin.conf</code> can be used to determine the cluster name kube-apiserver address.</p> <pre><code># create a new kubeconfig with the server information\nkubectl config set-cluster $CLUSTER_NAME --kubeconfig=/tmp/rabbit/rabbit.conf --server=$SERVER_ADDRESS --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true\n# add the key and cert for this user to the config\nkubectl config set-credentials $USERNAME --kubeconfig=/tmp/rabbit/rabbit.conf --client-certificate=/tmp/rabbit/rabbit.crt --client-key=/tmp/rabbit/rabbit.key --embed-certs=true\n# add a context\nkubectl config set-context $USERNAME --kubeconfig=/tmp/rabbit/rabbit.conf --cluster=$CLUSTER_NAME --user=$USERNAME\n</code></pre> <p>The kubeconfig file should be placed in a location where HPE employees have read access to it.</p>"},{"location":"guides/rbac-for-users/readme/#create-clusterrole-and-clusterrolebinding","title":"Create ClusterRole and ClusterRoleBinding","text":"<p>The next step is to create ClusterRole and ClusterRoleBinding resources. The ClusterRole provided allows viewing all cluster and namespace scoped resources, but disallows creating, deleting, or modifying any resources.</p> <p>ClusterRole <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\nname: hpe-viewer\nrules:\n- apiGroups: [ \"*\" ]\nresources: [ \"*\" ]\nverbs: [ get, list ]\n</code></pre></p> <p>ClusterRoleBinding <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\nname: hpe-viewer\nsubjects:\n- kind: User\nname: hpe\napiGroup: rbac.authorization.k8s.io\nroleRef:\nkind: ClusterRole\nname: hpe-viewer\napiGroup: rbac.authorization.k8s.io\n</code></pre></p> <p>Both of these resources can be created using the <code>kubectl apply</code> command.</p>"},{"location":"guides/rbac-for-users/readme/#testing","title":"Testing","text":"<p>Get, List, Create, Delete, and Modify operations can be tested as the \"hpe\" user by setting the KUBECONFIG environment variable to use the new kubeconfig file. Get and List should be the only allowed operations. Other operations should fail with a \"forbidden\" error.</p> <pre><code>export KUBECONFIG=/tmp/hpe/hpe.conf\n</code></pre>"},{"location":"guides/rbac-for-users/readme/#rbac-for-workload-manager-wlm","title":"RBAC for Workload Manager (WLM)","text":"<p>Note This section assumes the reader has read and understood the steps described above for setting up <code>RBAC for Users</code>.</p> <p>A workload manager (WLM) such as Flux or Slurm will interact with DataWorkflowServices as a privileged user. RBAC is used to limit the operations that a WLM can perform on a Rabbit system.</p> <p>The following steps are required to create a user and a role for the WLM.  In this case, we're creating a user to be used with the Flux WLM:</p> <ul> <li>Generate a new key/cert pair for a \"flux\" user</li> <li>Creating a new kubeconfig file</li> <li>Adding RBAC rules for the \"flux\" user to allow appropriate access to the DataWorkflowServices API.</li> </ul>"},{"location":"guides/rbac-for-users/readme/#generate-a-key-and-certificate_1","title":"Generate a Key and Certificate","text":"<p>Generate a key and certificate for our \"flux\" user, similar to the way we created one for the \"hpe\" user above.  Substitute \"flux\" in place of \"hpe\".</p>"},{"location":"guides/rbac-for-users/readme/#create-a-kubeconfig_1","title":"Create a kubeconfig","text":"<p>After the keys have been generated, a new kubeconfig file can be created for the \"flux\" user, similar to the one for the \"hpe\" user above.  Again, substitute \"flux\" in place of \"hpe\".</p>"},{"location":"guides/rbac-for-users/readme/#apply-the-provided-clusterrole-and-create-a-clusterrolebinding","title":"Apply the provided ClusterRole and create a ClusterRoleBinding","text":"<p>DataWorkflowServices has already defined the role to be used with WLMs.  Simply apply the <code>workload-manager</code> ClusterRole from DataWorkflowServices to the system:</p> <pre><code>kubectl apply -f https://github.com/HewlettPackard/dws/raw/master/config/rbac/workload_manager_role.yaml\n</code></pre> <p>Create and apply a ClusterRoleBinding to associate the \"flux\" user with the <code>workload-manager</code> ClusterRole:</p> <p>ClusterRoleBinding <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\nname: flux\nsubjects:\n- kind: User\nname: flux\napiGroup: rbac.authorization.k8s.io\nroleRef:\nkind: ClusterRole\nname: workload-manager\napiGroup: rbac.authorization.k8s.io\n</code></pre></p> <p>The WLM should then use the kubeconfig file associated with this \"flux\" user to access the DataWorkflowServices API and the Rabbit system.</p>"},{"location":"guides/storage-profiles/readme/","title":"Storage Profile Overview","text":"<p>Storage Profiles allow for customization of the Rabbit storage provisioning process. Examples of content that can be customized via storage profiles is</p> <ol> <li>The RAID type used for storage</li> <li>Any mkfs or LVM args used</li> <li>An external MGS NID for Lustre</li> <li>A boolean value indicating the Lustre MGT and MDT should be combined on the same target device </li> </ol> <p>DW directives that allocate storage on Rabbit nodes allow a <code>profile</code> parameter to be specified to control how the storage is configured. NNF software provides a set of canned profiles to choose from, and the administrator may create more profiles.</p> <p>The administrator shall choose one profile to be the default profile that is used when a profile parameter is not specified.</p>"},{"location":"guides/storage-profiles/readme/#specifying-a-profile","title":"Specifying a Profile","text":"<p>To specify a profile name on a #DW directive, use the <code>profile</code> option <pre><code>#DW jobdw type=lustre profile=durable capacity=5GB name=example\n</code></pre></p>"},{"location":"guides/storage-profiles/readme/#setting-a-default-profile","title":"Setting A Default Profile","text":"<p>A default profile must be defined at all times. Any #DW line that does not specify a profile will use the default profile. If a default profile is not defined, then any new workflows will be rejected. If more than one profile is marked as default then any new workflows will be rejected.</p> <p>To query existing profiles</p> <pre><code>$ kubectl get nnfstorageprofiles -A\nNAMESPACE    NAME          DEFAULT   AGE\nnnf-system   durable       true      14s\nnnf-system   performance   false     6s\n</code></pre> <p>To set the default flag on a profile <pre><code>$ kubectl patch nnfstorageprofile performance -n nnf-system --type merge -p '{\"data\":{\"default\":true}}'\n</code></pre></p> <p>To clear the default flag on a profile <pre><code>$ kubectl patch nnfstorageprofile durable -n nnf-system --type merge -p '{\"data\":{\"default\":false}}'\n</code></pre></p>"},{"location":"guides/storage-profiles/readme/#profile-parameters","title":"Profile Parameters","text":""},{"location":"guides/storage-profiles/readme/#xfs","title":"XFS","text":"<p>The following shows how to specify command line options for pvcreate, vgcreate, lvcreate, and mkfs for XFS storage. Optional mount options are specified one per line</p> <pre><code>apiVersion: nnf.cray.hpe.com/v1alpha1\nkind: NnfStorageProfile\nmetadata:\nname: xfs-stripe-example\nnamespace: nnf-system\ndata:\n[...]\nxfsStorage:\ncommandlines:\npvCreate: $DEVICE\nvgCreate: $VG_NAME $DEVICE_LIST\nlvCreate: -l 100%VG --stripes $DEVICE_NUM --stripesize=32KiB --name $LV_NAME $VG_NAME\nmkfs: $DEVICE\noptions:\nmountRabbit:\n- noatime\n- nodiratime\n[...]\n</code></pre>"},{"location":"guides/storage-profiles/readme/#gfs2","title":"GFS2","text":"<p>The following shows how to specify command line options for pvcreate, lvcreate, and mkfs for GFS2.</p> <pre><code>apiVersion: nnf.cray.hpe.com/v1alpha1\nkind: NnfStorageProfile\nmetadata:\nname: gfs2-stripe-example\nnamespace: nnf-system\ndata:\n[...]\ngfs2Storage:\ncommandlines:\npvCreate: $DEVICE\nvgCreate: $VG_NAME $DEVICE_LIST\nlvCreate: -l 100%VG --stripes $DEVICE_NUM --stripesize=32KiB --name $LV_NAME $VG_NAME\nmkfs: -j2 -p $PROTOCOL -t $CLUSTER_NAME:$LOCK_SPACE $DEVICE\n[...]\n</code></pre>"},{"location":"guides/storage-profiles/readme/#lustre-zfs","title":"Lustre / ZFS","text":"<p>The following shows how to specify a zpool virtual device (vdev). In this case the default vdev is a stripe. See zpoolconcepts(7) for virtual device descriptions.</p> <pre><code>apiVersion: nnf.cray.hpe.com/v1alpha1\nkind: NnfStorageProfile\nmetadata:\nname: zpool-stripe-example\nnamespace: nnf-system\ndata:\n[...]\nlustreStorage:\nmgtCommandlines:\nzpoolCreate: -O canmount=off -o cachefile=none $POOL_NAME $DEVICE_LIST\nmkfs: --mgs $VOL_NAME\nmdtCommandlines:\nzpoolCreate: -O canmount=off -o cachefile=none $POOL_NAME $DEVICE_LIST\nmkfs: --mdt --fsname=$FS_NAME --mgsnode=$MGS_NID --index=$INDEX $VOL_NAME\nmgtMdtCommandlines:\nzpoolCreate: -O canmount=off -o cachefile=none $POOL_NAME $DEVICE_LIST\nmkfs: --mgs --mdt --fsname=$FS_NAME --index=$INDEX $VOL_NAME\nostCommandlines:\nzpoolCreate: -O canmount=off -o cachefile=none $POOL_NAME $DEVICE_LIST\nmkfs: --ost --fsname=$FS_NAME --mgsnode=$MGS_NID --index=$INDEX $VOL_NAME\n[...]\n</code></pre>"},{"location":"guides/storage-profiles/readme/#zfs-dataset-properties","title":"ZFS dataset properties","text":"<p>The following shows how to specify ZFS dataset properties in the <code>--mkfsoptions</code> arg for mkfs.lustre. See zfsprops(7).</p> <pre><code>apiVersion: nnf.cray.hpe.com/v1alpha1\nkind: NnfStorageProfile\nmetadata:\nname: zpool-stripe-example\nnamespace: nnf-system\ndata:\n[...]\nlustreStorage:\n[...]\nostCommandlines:\nzpoolCreate: -O canmount=off -o cachefile=none $POOL_NAME $DEVICE_LIST\nmkfs: --ost --mkfsoptions=\"recordsize=1024K -o compression=lz4\" --fsname=$FS_NAME --mgsnode=$MGS_NID --index=$INDEX $VOL_NAME\n[...]\n</code></pre>"},{"location":"guides/storage-profiles/readme/#mount-options-for-targets","title":"Mount Options for Targets","text":""},{"location":"guides/storage-profiles/readme/#persistent-mount-options","title":"Persistent Mount Options","text":"<p>Use the mkfs.lustre <code>--mountfsoptions</code> parameter to set persistent mount options for Lustre targets.</p> <pre><code>apiVersion: nnf.cray.hpe.com/v1alpha1\nkind: NnfStorageProfile\nmetadata:\nname: target-mount-option-example\nnamespace: nnf-system\ndata:\n[...]\nlustreStorage:\n[...]\nostCommandlines:\nzpoolCreate: -O canmount=off -o cachefile=none $POOL_NAME $DEVICE_LIST\nmkfs: --ost --mountfsoptions=\"errors=remount-ro,mballoc\" --mkfsoptions=\"recordsize=1024K -o compression=lz4\" --fsname=$FS_NAME --mgsnode=$MGS_NID --index=$INDEX $VOL_NAME\n[...]\n</code></pre>"},{"location":"guides/storage-profiles/readme/#non-persistent-mount-options","title":"Non-Persistent Mount Options","text":"<p>Non-persistent mount options can be specified with the ostOptions.mountTarget parameter to the NnfStorageProfile:</p> <pre><code>apiVersion: nnf.cray.hpe.com/v1alpha1\nkind: NnfStorageProfile\nmetadata:\nname: target-mount-option-example\nnamespace: nnf-system\ndata:\n[...]\nlustreStorage:\n[...]\nostCommandlines:\nzpoolCreate: -O canmount=off -o cachefile=none $POOL_NAME $DEVICE_LIST\nmkfs: --ost --mountfsoptions=\"errors=remount-ro\" --mkfsoptions=\"recordsize=1024K -o compression=lz4\" --fsname=$FS_NAME --mgsnode=$MGS_NID --index=$INDEX $VOL_NAME\nostOptions:\nmountTarget:\n- mballoc\n[...]\n</code></pre>"},{"location":"guides/storage-profiles/readme/#command-line-variables","title":"Command Line Variables","text":""},{"location":"guides/storage-profiles/readme/#pvcreate","title":"pvcreate","text":"<ul> <li><code>$DEVICE</code> - expands to the <code>/dev/&lt;path&gt;</code> value for one device that has been allocated</li> </ul>"},{"location":"guides/storage-profiles/readme/#vgcreate","title":"vgcreate","text":"<ul> <li><code>$VG_NAME</code> - expands to a volume group name that is controlled by Rabbit software.</li> <li><code>$DEVICE_LIST</code> - expands to a list of space-separated <code>/dev/&lt;path&gt;</code> devices. This list will contain the devices that were iterated over for the pvcreate step.</li> </ul>"},{"location":"guides/storage-profiles/readme/#lvcreate","title":"lvcreate","text":"<ul> <li><code>$VG_NAME</code> - see vgcreate above.</li> <li><code>$LV_NAME</code> - expands to a logical volume name that is controlled by Rabbit software.</li> <li><code>$DEVICE_NUM</code> - expands to a number indicating the number of devices allocated for the volume group.</li> <li><code>$DEVICE1, $DEVICE2, ..., $DEVICEn</code> - each expands to one of the devices from the <code>$DEVICE_LIST</code> above.</li> </ul>"},{"location":"guides/storage-profiles/readme/#xfs-mkfs","title":"XFS mkfs","text":"<ul> <li><code>$DEVICE</code> - expands to the <code>/dev/&lt;path&gt;</code> value for the logical volume that was created by the lvcreate step above.</li> </ul>"},{"location":"guides/storage-profiles/readme/#gfs2-mkfs","title":"GFS2 mkfs","text":"<ul> <li><code>$DEVICE</code> - expands to the <code>/dev/&lt;path&gt;</code> value for the logical volume that was created by the lvcreate step above.</li> <li><code>$CLUSTER_NAME</code> - expands to a cluster name that is controlled by Rabbit Software</li> <li><code>$LOCK_SPACE</code> - expands to a lock space key that is controlled by Rabbit Software.</li> <li><code>$PROTOCOL</code> - expands to a locking protocol that is controlled by Rabbit Software.</li> </ul>"},{"location":"guides/storage-profiles/readme/#zpool-create","title":"zpool create","text":"<ul> <li><code>$DEVICE_LIST</code> - expands to a list of space-separated <code>/dev/&lt;path&gt;</code> devices. This list will contain the devices that were allocated for this storage request.</li> <li><code>$POOL_NAME</code> - expands to a pool name that is controlled by Rabbit software.</li> <li><code>$DEVICE_NUM</code> - expands to a number indicating the number of devices allocated for this storage request.</li> <li><code>$DEVICE1, $DEVICE2, ..., $DEVICEn</code> - each expands to one of the devices from the <code>$DEVICE_LIST</code> above.</li> </ul>"},{"location":"guides/storage-profiles/readme/#lustre-mkfs","title":"lustre mkfs","text":"<ul> <li><code>$FS_NAME</code> - expands to the filesystem name that was passed to Rabbit software from the workflow's #DW line.</li> <li><code>$MGS_NID</code> - expands to the NID of the MGS. If the MGS was orchestrated by nnf-sos then an appropriate internal value will be used.</li> <li><code>$POOL_NAME</code> - see zpool create above.</li> <li><code>$VOL_NAME</code> - expands to the volume name that will be created. This value will be <code>&lt;pool_name&gt;/&lt;dataset&gt;</code>, and is controlled by Rabbit software.</li> <li><code>$INDEX</code> - expands to the index value of the target and is controlled by Rabbit software.</li> </ul>"},{"location":"guides/user-containers/readme/","title":"NNF User Containers","text":"<p>NNF User Containers are a mechanism to allow user-defined containerized applications to be run on Rabbit nodes with access to NNF ephemeral and persistent storage.</p> <p>Note</p> <p>The following is a limited look at User Containers.  More content will be provided after the RFC has been finalized.</p>"},{"location":"guides/user-containers/readme/#custom-nnfcontainerprofile","title":"Custom NnfContainerProfile","text":"<p>The author of a containerized application will work with the administrator to define a pod specification template for the container and to create an appropriate NnfContainerProfile resource for the container.  The image and tag for the user's container will be specified in the profile.</p> <p>New NnfContainerProfile resources may be created by copying one of the provided example profiles from the <code>nnf-system</code> namespace.  The examples may be found by listing them with <code>kubectl</code>:</p> <pre><code>kubectl get nnfcontainerprofiles -n nnf-system\n</code></pre>"},{"location":"guides/user-containers/readme/#workflow-job-specification","title":"Workflow Job Specification","text":"<p>The user's workflow will specify the name of the NnfContainerProfile in a DW directive.  If the custom profile is named <code>red-rock-slushy</code> then it will be specified in the \"#DW container\" directive with the \"profile\" parameter.</p> <pre><code>#DW container profile=red-rock-slushy  [...]\n</code></pre>"},{"location":"guides/user-containers/readme/#using-a-private-container-repository","title":"Using a Private Container Repository","text":"<p>The user's containerized application may be placed in a private repository.  In this case, the user must define an access token to be used with that repository, and that token must be made available to the Rabbit's Kubernetes environment so that it can pull that container from the private repository.</p> <p>See Pull an Image from a Private Registry in the Kubernetes documentation for more information.</p>"},{"location":"guides/user-containers/readme/#about-the-example","title":"About the Example","text":"<p>Each container registry will have its own way of letting its users create tokens to be used with their repositories.  Docker Hub will be used for the private repository in this example, and the user's account on Docker Hub will be \"dean\".</p>"},{"location":"guides/user-containers/readme/#preparing-the-private-repository","title":"Preparing the Private Repository","text":"<p>The user's application container is named \"red-rock-slushy\".  To store this container on Docker Hub the user must log into docker.com with their browser and click the \"Create repository\" button to create a repository named \"red-rock-slushy\", and the user must check the box that marks the repository as private.  The repository's name will be displayed as \"dean/red-rock-slushy\" with a lock icon to show that it is private.</p>"},{"location":"guides/user-containers/readme/#create-and-push-a-container","title":"Create and Push a Container","text":"<p>The user will create their container image in the usual ways, naming it for their private repository and tagging it according to its release.</p> <p>Prior to pushing images to the repository, the user must complete a one-time login to the Docker registry using the docker command-line tool.</p> <pre><code>docker login -u dean\n</code></pre> <p>After completing the login, the user may then push their images to the repository.</p> <pre><code>docker push dean/red-rock-slushy:v1.0\n</code></pre>"},{"location":"guides/user-containers/readme/#generate-a-read-only-token","title":"Generate a Read-Only Token","text":"<p>A read-only token must be generated to allow Kubernetes to pull that container image from the private repository, because Kubernetes will not be running as that user.  This token must be given to the administrator, who will use it to create a Kubernetes secret.</p> <p>To log in and generate a read-only token to share with the administrator, the user must follow these steps:</p> <ul> <li>Visit docker.com and log in using their browser.</li> <li>Click on the username in the upper right corner.</li> <li>Select \"Account Settings\" and navigate to \"Security\".</li> <li>Click the \"New Access Token\" button to create a read-only token.</li> <li>Keep a copy of the generated token to share with the administrator.</li> </ul>"},{"location":"guides/user-containers/readme/#store-the-read-only-token-as-a-kubernetes-secret","title":"Store the Read-Only Token as a Kubernetes Secret","text":"<p>The adminstrator must store the user's read-only token as a kubernetes secret.  The secret must be placed in the <code>default</code> namespace, which is the same namespace where the user containers will be run.  The secret must include the user's Docker Hub username and the email address they have associated with that username.  In this case, the secret will be named <code>readonly-red-rock-slushy</code>.</p> <pre><code>$ USER_TOKEN=users-token-text\n$ USER_NAME=dean\n$ USER_EMAIL=dean@myco.com\n$ SECRET_NAME=readonly-red-rock-slushy\n$ kubectl create secret docker-registry $SECRET_NAME -n default --docker-server=\"https://index.docker.io/v1/\" --docker-username=$USER_NAME --docker-password=$USER_TOKEN --docker-email=$USER_EMAIL\n</code></pre>"},{"location":"guides/user-containers/readme/#add-the-secret-to-the-nnfcontainerprofile","title":"Add the Secret to the NnfContainerProfile","text":"<p>The administrator must add an <code>imagePullSecrets</code> list to the NnfContainerProfile resource that was created for this user's containerized application.</p> <p>The following profile shows the placement of the <code>readonly-red-rock-slushy</code> secret which was created in the previous step, and points to the user's <code>dean/red-rock-slushy:v1.0</code> container.</p> <pre><code>apiVersion: nnf.cray.hpe.com/v1alpha1\nkind: NnfContainerProfile\nmetadata:\nname: red-rock-slushy\nnamespace: nnf-system\ndata:\npinned: false\nretryLimit: 6\nspec:\nimagePullSecrets:\n- name: readonly-red-rock-slushy\ncontainers:\n- command:\n- /users-application\nimage: dean/red-rock-slushy:v1.0\nname: red-rock-app\nstorages:\n- name: DW_JOB_foo_local_storage\noptional: false\n- name: DW_PERSISTENT_foo_persistent_storage\noptional: true\n</code></pre> <p>Now any user can select this profile in their Workflow by specifying it in a <code>#DW container</code> directive.</p> <pre><code>#DW container profile=red-rock-slushy  [...]\n</code></pre>"},{"location":"guides/user-containers/readme/#using-a-private-container-repository-for-mpi-application-containers","title":"Using a Private Container Repository for MPI Application Containers","text":"<p>If our user's containerized application instead contains an MPI application, because perhaps it's a private copy of nnf-mfu, then the administrator would insert two <code>imagePullSecrets</code> lists into the <code>mpiSpec</code> of the NnfContainerProfile for the MPI launcher and the MPI worker.</p> <pre><code>apiVersion: nnf.cray.hpe.com/v1alpha1\nkind: NnfContainerProfile\nmetadata:\nname: mpi-red-rock-slushy\nnamespace: nnf-system\ndata:\nmpiSpec:\nmpiImplementation: OpenMPI\nmpiReplicaSpecs:\nLauncher:\ntemplate:\nspec:\nimagePullSecrets:\n- name: readonly-red-rock-slushy\ncontainers:\n- command:\n- mpirun\n- dcmp\n- $(DW_JOB_foo_local_storage)/0\n- $(DW_JOB_foo_local_storage)/1\nimage: dean/red-rock-slushy:v2.0\nname: red-rock-launcher\nWorker:\ntemplate:\nspec:\nimagePullSecrets:\n- name: readonly-red-rock-slushy\ncontainers:\n- image: dean/red-rock-slushy:v2.0\nname: red-rock-worker\nrunPolicy:\ncleanPodPolicy: Running\nsuspend: false\nslotsPerWorker: 1\nsshAuthMountPath: /root/.ssh\npinned: false\nretryLimit: 6\nstorages:\n- name: DW_JOB_foo_local_storage\noptional: false\n- name: DW_PERSISTENT_foo_persistent_storage\noptional: true\n</code></pre> <p>Now any user can select this profile in their Workflow by specifying it in a <code>#DW container</code> directive.</p> <pre><code>#DW container profile=mpi-red-rock-slushy  [...]\n</code></pre>"},{"location":"rfcs/","title":"Request for Comment","text":"<ol> <li> <p>Rabbit Request For Comment Process  - Published</p> </li> <li> <p>Rabbit Storage For Containerized Applications  - Discussion</p> </li> </ol>"},{"location":"rfcs/0001/readme/","title":"Rabbit Request For Comment Process","text":"<p>Rabbit software must be designed in close collaboration with our end-users. Part of this process involves open discussion in the form of Request For Comment (RFC) documents. The remainder of this document presents the RFC process for Rabbit.</p>"},{"location":"rfcs/0001/readme/#history-philosophy","title":"History &amp; Philosophy","text":"<p>NNF RFC documents are modeled after the long history of IETF RFC documents that describe the internet. The philosophy is captured best in RFC 3</p> <p>The content of a [...] note may be any thought, suggestion, etc. related to the HOST software or other aspect of the network.  Notes are encouraged to be timely rather than polished.  Philosophical positions without examples or other specifics, specific suggestions or implementation techniques without introductory or background explication, and explicit questions without any attempted answers are all acceptable.  The minimum length for a [...] note is one sentence.</p> <p>These standards (or lack of them) are stated explicitly for two reasons. First, there is a tendency to view a written statement as ipso facto authoritative, and we hope to promote the exchange and discussion of considerably less than authoritative ideas.  Second, there is a natural hesitancy to publish something unpolished, and we hope to ease this inhibition.</p>"},{"location":"rfcs/0001/readme/#when-to-create-an-rfc","title":"When to Create an RFC","text":"<p>New features, improvements, and other tasks that need to source feedback from multiple sources are to be written as Request For Comment (RFC) documents.</p>"},{"location":"rfcs/0001/readme/#metadata","title":"Metadata","text":"<p>At the start of each RFC, there must include a short metadata block that contains information useful for filtering and sorting existing documents. This markdown is not visible inside the document.</p> <pre><code>---\nauthors: John Doe &lt;john.doe@company.com&gt;, Jane Doe &lt;jane.doe@company.com&gt;\nstate: prediscussion|ideation|discussion|published|committed|abandoned\ndiscussion: (link to PR, if available)\n----\n</code></pre>"},{"location":"rfcs/0001/readme/#creation","title":"Creation","text":"<p>An RFC should be created at the next freely available 4-digit index the GitHub RFC folder. Create a folder for your RFC and write your RFC document as <code>readme.md</code> using standard Markdown. Include additional documents or images in the folder if needed.</p> <p>Add an entry to <code>/docs/rfcs/index.md</code></p> <p>Add an entry to <code>/mkdocs.yml</code> in the <code>nav[RFCs]</code> section</p>"},{"location":"rfcs/0001/readme/#push","title":"Push","text":"<p>Push your changes to your RFC branch</p> <pre><code>git add --all\ngit commit -s -m \"[####]: Your Request For Comment Document\"\ngit push origin ####\n</code></pre>"},{"location":"rfcs/0001/readme/#pull-request","title":"Pull Request","text":"<p>Submit a PR for your branch. This will open your RFC to comments. Add those individuals who are interested in your RFC as reviewers.</p>"},{"location":"rfcs/0001/readme/#merge","title":"Merge","text":"<p>Once consensus has been reached on your RFC, merge to main origin. </p>"},{"location":"rfcs/0002/readme/","title":"Rabbit storage for containerized applications","text":"<p>For Rabbit to provide storage to a containerized application there needs to be some mechanism. The remainder of this RFC proposes that mechanism.</p>"},{"location":"rfcs/0002/readme/#actors","title":"Actors","text":"<p>There are several actors involved:</p> <ul> <li>The AUTHOR of the containerized application</li> <li>The ADMINISTRATOR who works with the author to determine the application requirements for execution</li> <li>The USER who intends to use the application using the 'container' directive in their job specification</li> <li>The RABBIT software that interprets the #DWs and starts the container during execution of the job</li> </ul> <p>There are multiple relationships between the actors:</p> <ul> <li>AUTHOR to ADMINISTRATOR: The author tells the administrator how their application is executed and the NNF storage requirements.</li> <li>Between the AUTHOR and USER: The application expects certain storage, and the #DW must meet those expectations.</li> <li>ADMINISTRATOR to RABBIT: Admin tells Rabbit how to run the containerized application with the required storage.</li> <li>Between USER and RABBIT: User provides the #DW container directive in the job specification. Rabbit validates and interprets the directive.</li> </ul>"},{"location":"rfcs/0002/readme/#proposal","title":"Proposal","text":"<p>The proposal below outlines the high level behavior of running containers in a workflow:</p> <ol> <li>The AUTHOR writes their application expecting NNF Storage at specific locations. For each storage requirement, they define:<ol> <li>a unique name for the storage which can be referenced in the 'container' directive</li> <li>the required mount path or mount path prefix</li> <li>other constraints or storage requirements (e.g. minimum capacity)</li> </ol> </li> <li>The AUTHOR works with the ADMINISTRATOR to define:<ol> <li>a unique name for the program to be referred by USER</li> <li>the pod template or MPI Job specification for executing their program</li> <li>the NNF storage requirements described above.</li> </ol> </li> <li>The ADMINISTRATOR creates a corresponding NNF Container Profile Kubernetes custom resource with the necessary NNF storage requirements and pod specification as described by the AUTHOR</li> <li>The USER who desires to use the application works with the AUTHOR and the related NNF Container Profile to understand the storage requirements</li> <li>The USER submits a WLM job with the #DW container directive variables populated</li> <li>WLM runs the workflow and drives it through the following stages...<ol> <li><code>Proposal</code>: RABBIT validates the #DW container directive by comparing the supplied values to those listed in the NNF Container Profile. If the workflow fails to meet the requirements, the job fails</li> <li><code>PreRun</code>: RABBIT software:<ol> <li>duplicates the pod template specification from the Container Profile and patches the necessary Volumes and the config map. The spec is used as the basis for starting the necessary pods and containers</li> <li>creates a config map reflecting the storage requirements and any runtime parameters; this is provided to the container at the volume mount named <code>nnf-config</code>, if specified</li> </ol> </li> <li>The containerized application(s) executes. The expected mounts are available per the requirements and celebration occurs. The pods continue to run until:</li> <li>a pod completes successfully (any failed pods will be retried)</li> <li>the max number of pod retries is hit (indicating failure on all retry attempts)<ol> <li>Note: retry limit is non-optional per Kubernetes configuration</li> <li>If retries are not desired, this number could be set to 0 to disable any retry attempts</li> </ol> </li> <li><code>PostRun</code>: RABBIT software:</li> <li>marks the stage as <code>Ready</code> if the pods have all completed successfully. This includes a successful retry after preceding failures</li> <li>starts a timer for any running pods. Once the timeout is hit, the pods will be killed and the workflow will indicate failure</li> <li>leaves all pods around for log inspection</li> </ol> </li> </ol>"},{"location":"rfcs/0002/readme/#container-assignment-to-rabbit-nodes","title":"Container Assignment to Rabbit Nodes","text":"<p>During <code>Proposal</code>, the USER must assign compute nodes for the container workflow. The assigned compute nodes determine which Rabbit nodes run the containers.</p>"},{"location":"rfcs/0002/readme/#container-definition","title":"Container Definition","text":"<p>Containers can be launched in two ways:</p> <ol> <li>MPI Jobs</li> <li>Non-MPI Jobs</li> </ol> <p>MPI Jobs are launched using <code>mpi-operator</code>. This uses a launcher/worker model. The launcher pod is responsible for running the <code>mpirun</code> command that will target the worker pods to run the MPI application. The launcher will run on the first targeted NNF node and the workers will run on each of the targeted NNF nodes.</p> <p>For Non-MPI jobs, <code>mpi-operator</code> is not used. This model runs the same application on each of the targeted NNF nodes.</p> <p>The NNF Container Profile allows a user to pick one of these methods. Each method is defined in similar, but different fashions. Since MPI Jobs use <code>mpi-operator</code>, the <code>MPIJobSpec</code> is used to define the container(s). For Non-MPI Jobs a <code>PodSpec</code> is used to define the container(s).</p> <p>An example of an MPI Job is below. The <code>data.mpiSpec</code> field is defined:</p> <pre><code>kind: NnfContainerProfile\napiVersion: nnf.cray.hpe.com/v1alpha1\ndata:\nmpiSpec:\nmpiReplicaSpecs:\nLauncher:\ntemplate:\nspec:\ncontainers:\n- command:\n- mpirun\n- dcmp\n- $(DW_JOB_foo_local_storage)/0\n- $(DW_JOB_foo_local_storage)/1\nimage: ghcr.io/nearnodeflash/nnf-mfu:latest\nname: example-mpi\nWorker:\ntemplate:\nspec:\ncontainers:\n- image: ghcr.io/nearnodeflash/nnf-mfu:latest\nname: example-mpi\nslotsPerWorker: 1\n...\n</code></pre> <p>An example of a Non-MPI Job is below. The <code>data.spec</code> field is defined:</p> <pre><code>kind: NnfContainerProfile\napiVersion: nnf.cray.hpe.com/v1alpha1\ndata:\nspec:\ncontainers:\n- command:\n- /bin/sh\n- -c\n- while true; do date &amp;&amp; sleep 5; done\nimage: alpine:latest\nname: example-forever\n...\n</code></pre> <p>In both cases, the <code>spec</code> is used as a starting point to define the containers. NNF software supplements the specification to add functionality (e.g. mounting #DW storages). In other words, what you see here will not be the final spec for the container that ends up running as part of the container workflow.</p>"},{"location":"rfcs/0002/readme/#security","title":"Security","text":"<p>The workflow's UID and GID are used to run the container application and for mounting the specified fileystems in the container. Kubernetes allows for a way to define permissions for a container using a Security Context.</p> <p><code>mpirun</code> uses <code>ssh</code> to communicate with the worker nodes. <code>ssh</code> requires that UID is assigned to a username. Since the UID/GID are dynamic values from the workflow, work must be done to the container's <code>/etc/passwd</code> to map the UID/GID to a username. An <code>InitContainer</code> is used to modify <code>/etc/passwd</code> and mount it into the container.</p>"},{"location":"rfcs/0002/readme/#communication-details","title":"Communication Details","text":"<p>The following subsections outline the proposed communication between the Rabbit nodes themselves and the Compute nodes.</p>"},{"location":"rfcs/0002/readme/#rabbit-to-rabbit-communication","title":"Rabbit-to-Rabbit Communication","text":""},{"location":"rfcs/0002/readme/#non-mpi-jobs","title":"Non-MPI Jobs","text":"<p>Each rabbit node can be reached via <code>&lt;hostname&gt;.&lt;subdomain&gt;</code> using DNS. The hostname is the Rabbit node name and the workflow name is used for the subdomain.</p> <p>For example, a workflow name of <code>foo</code> that targets <code>rabbit-node2</code> would be <code>rabbit-node2.foo</code>.</p> <p>Environment variables are provided to the container and ConfigMap for each rabbit that is targeted by the container workflow:</p> <pre><code>NNF_CONTAINER_NODES=rabbit-node2 rabbit-node3\nNNF_CONTAINER_SUBDOMAIN=foo\nNNF_CONTAINER_DOMAIN=default.svc.cluster.local\n</code></pre> <pre><code>kind: ConfigMap\napiVersion: v1\ndata:\nnnfContainerNodes:\n- rabbit-node2\n- rabbit-node3\nnnfContainerSubdomain: foo\nnnfContainerDomain: default.svc.cluster.local\n</code></pre> <p>DNS can then be used to communicate with other Rabbit containers. The FQDN for the container running on rabbit-node2 is <code>rabbit-node2.foo.default.svc.cluster.local</code>.</p>"},{"location":"rfcs/0002/readme/#mpi-jobs","title":"MPI Jobs","text":"<p>For MPI Jobs, these hostnames and subdomains will be slightly different due to the implementation of <code>mpi-operator</code>. However, the variables will remain the same and provide a consistent way to retrieve the values.</p>"},{"location":"rfcs/0002/readme/#compute-to-rabbit-communication","title":"Compute-to-Rabbit Communication","text":"<p>For Compute to Rabbit communication, the proposal is to use an open port between the nodes, so the applications could communicate using IP protocol.  The port number would be assigned by the Rabbit software and included in the workflow resource's environmental variables after the Setup state (similar to workflow name &amp; namespace).  Flux should provide the port number to the compute application via an environmental variable or command line argument. The containerized application would always see the same port number using the <code>hostPort</code>/<code>containerPort</code> mapping functionality included in Kubernetes. To clarify, the Rabbit software is picking and managing the ports picked for <code>hostPort</code>.</p> <p>This requires a range of ports to be open in the firewall configuration and specified in the rabbit system configuration. The fewer the number of ports available increases the chances of a port reservation conflict that would fail a workflow.</p> <p>Example port range definition in the SystemConfiguration:</p> <pre><code>apiVersion: v1\nitems:\n- apiVersion: dws.cray.hpe.com/v1alpha1\nkind: SystemConfiguration\nname: default\nnamespace: default\nspec:\ncontainerHostPortRangeMin: 30000\ncontainerHostPortRangeMax: 40000\n...\n</code></pre>"},{"location":"rfcs/0002/readme/#example","title":"Example","text":"<p>For this example, let's assume I've authored an application called <code>foo</code>. This application requires Rabbit local GFS2 storage and a persistent Lustre storage volume.</p> <p>Working with an administrator, my application's storage requirements and pod specification are placed in an NNF Container Profile <code>foo</code>:</p> <pre><code>kind: NnfContainerProfile\napiVersion: v1alpha1\nmetadata:\nname: foo\nnamespace: default\nspec:\npostRunTimeout: 300\nmaxRetries: 6\nstorages:\n- name: DW_JOB_foo-local-storage\noptional: false\n- name: DW_PERSISTENT_foo-persistent-storage\noptional: false\nspec:\ncontainers:\n- name: foo\nimage: foo:latest\ncommand:\n- /foo\nports:\n- name: compute\ncontainerPort: 80\n</code></pre> <p>Say Peter wants to use <code>foo</code> as part of his job specification. Peter would submit the job with the directives below:</p> <pre><code>#DW jobdw name=my-gfs2 type=gfs2 capacity=1TB\n\n#DW persistentdw name=some-lustre\n\n#DW container name=my-foo profile=foo                 \\\n    DW_JOB_foo-local-storage=my-gfs2                  \\\n    DW_PERSISTENT_foo-persistent-storage=some-lustre\n</code></pre> <p>Since the NNF Container Profile has specified that both storages are not optional (i.e. <code>optional: false</code>), they must both be present in the #DW directives along with the <code>container</code> directive. Alternatively, if either was marked as optional (i.e. <code>optional: true</code>), it would not be required to be present in the #DW directives and therefore would not be mounted into the container.</p> <p>Peter submits the job to the WLM. WLM guides the job through the workflow states:</p> <ol> <li>Proposal: Rabbit software verifies the #DW directives. For the container directive <code>my-foo</code> with profile <code>foo</code>, the storage requirements listed in the NNF Container Profile are <code>foo-local-storage</code> and <code>foo-persistent-storage</code>. These values are correctly represented by the directive so it is valid.</li> <li>Setup: Since there is a jobdw, <code>my-gfs2</code>, Rabbit software provisions this storage.</li> <li> <p>Pre-Run:</p> <ol> <li> <p>Rabbit software generates a config map that corresponds to the storage requirements and runtime parameters.</p> <pre><code>    kind: ConfigMap\napiVersion: v1\nmetadata:\nname: my-job-container-my-foo\ndata:\nDW_JOB_foo_local_storage:             mount-type=indexed-mount\nDW_PERSISTENT_foo_persistent_storage: mount-type=mount-point\n...\n</code></pre> </li> <li> <p>Rabbit software creates a pod and duplicates the <code>foo</code> pod spec in the NNF Container Profile and fills in the necessary volumes and config map.</p> <pre><code>    kind: Pod\napiVersion: v1\nmetadata:\nname: my-job-container-my-foo\ntemplate:\nmetadata:\nname: foo\nnamespace: default\nspec:\ncontainers:\n# This section unchanged from Container Profile\n- name: foo\nimage: foo:latest\ncommand:\n- /foo\nvolumeMounts:\n- name: foo-local-storage\nmountPath: &lt;MOUNT_PATH&gt;\n- name: foo-persistent-storage\nmountPath: &lt;MOUNT_PATH&gt;\n- name: nnf-config\nmountPath: /nnf/config\nports:\n- name: compute\nhostPort: 9376 # hostport selected by Rabbit software\ncontainerPort: 80\n# volumes added by Rabbit software\nvolumes:\n- name: foo-local-storage\nhostPath:\npath: /nnf/job/my-job/my-gfs2\n- name: foo-persistent-storage\nhostPath:\npath: /nnf/persistent/some-lustre\n- name: nnf-config\nconfigMap:\nname: my-job-container-my-foo\n# securityContext added by Rabbit software - values will be inherited from the workflow\nsecurityContext:\nrunAsUser: 1000\nrunAsGroup: 2000\nfsGroup: 2000\n</code></pre> </li> <li> <p>Rabbit software starts the pods on Rabbit nodes</p> </li> <li>Post-Run</li> <li>Rabbit waits for all pods to finish (or until timeout is hit)</li> <li>If all pods are successful, Post-Run is marked as <code>Ready</code></li> <li>If any pod is not successful, Post-Run is not marked as <code>Ready</code></li> </ol> </li> </ol>"},{"location":"rfcs/0002/readme/#special-note-indexed-mount-type-for-gfs2-file-systems","title":"Special Note: Indexed-Mount Type for GFS2 File Systems","text":"<p>When using a GFS2 file system, each compute is allocated its own Rabbit volume. The Rabbit software mounts a collection of mount paths with a common prefix and an ending indexed value.</p> <p>Application AUTHORS must be aware that their desired mount-point really contains a collection of directories, one for each compute node. The mount point type can be known by consulting the config map values.</p> <p>If we continue the example from above, the <code>foo</code> application expects the foo-local-storage path of <code>/foo/local</code> to contain several directories</p> <pre><code>$ ls /foo/local/*\n\nnode-0\nnode-1\nnode-2\n...\nnode-N\n</code></pre> <p>Node positions are not absolute locations. WLM could, in theory, select 6 physical compute nodes at physical location 1, 2, 3, 5, 8, 13, which would appear as directories <code>/node-0</code> through <code>/node-5</code> in the container path.</p> <p>Symlinks will be added to support the physical compute node names. Assuming a compute node hostname of <code>compute-node-1</code> from the example above, it would link to <code>node-0</code>, <code>compute-node-2</code> would link to <code>node-1</code>, etc.</p> <p>Additionally, not all container instances could see the same number of compute nodes in an indexed-mount scenario. If 17 compute nodes are required for the job, WLM may assign 16 nodes to run one Rabbit, and 1 node to another Rabbit.</p>"}]}